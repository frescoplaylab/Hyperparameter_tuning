{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nF_5oOZ9OEaG"
   },
   "source": [
    "Welcome to the first handson!!!\n",
    "- In this handson you will be using the concept of GD with momentum, RMS prop and Adam prop to build optimized deep neural network\n",
    "- You will also be implementing minibatch gradient and L2 regularization to train you network\n",
    "- Follow the instruction provided for cell to write the code in each cell.\n",
    "- Run the below cell for to import necessary packages to read and visualize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is provided as file named 'data.csv'.  \n",
    "Using pandas read the csv file and assign the resulting dataframe to variable 'data'   \n",
    "for example if file name is 'xyz.csv' read file as **pd.read_csv('xyz.csv')** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "yaDtyHKHVAKL"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ivY_tF8yVKex"
   },
   "source": [
    "The data is provided as file named 'blobs.csv'.  \n",
    "Using pandas read the csv file and assign the resulting dataframe to variable 'data'   \n",
    "for example if file name is 'xyz.csv' read file as **pd.read_csv('xyz.csv')** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "0BHA1dAKyzw8"
   },
   "outputs": [],
   "source": [
    "###Start code here\n",
    "data =\n",
    "###End code here\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "IaRJ8HLLXpuK"
   },
   "source": [
    " Extract all the feature values from dataframe 'data' and assign it to variable 'X'\n",
    "- Extract target variable 'class' and assign it to variable 'y'.  \n",
    "Hint:\n",
    " - Use .values to exract values from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Start code here\n",
    "X = \n",
    "y = \n",
    "###End code\n",
    "\n",
    "assert X.shape == (10000, 10)\n",
    "assert Y.shape == (10000, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run the below cell to visualize the data in x-y plane. (visualization code has been written for you)\n",
    "- The green spots corresponds to target value 0 and green spots corresponds to target value 1\n",
    "- Though the data is more than 2 dimension only first two features are considered for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "OLSGMjm0VGkJ"
   },
   "outputs": [],
   "source": [
    "colors=['green','blue']\n",
    "cmap = matplotlib.colors.ListedColormap(colors)\n",
    "#Plot the figure\n",
    "plt.figure()\n",
    "plt.title('Non-linearly separable classes')\n",
    "plt.scatter(X[:,0], X[:,1], c=y,\n",
    "           marker= 'o', s=50,cmap=cmap,alpha = 0.5 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In order to feed the network the input has to be of **shape (number of features, number of samples)** and target should be of shape **(1, number of samples)**\n",
    "- Transpose X and assign it to variable 'X_data'\n",
    "- reshape y to have shape (1, number of samples) and assign to variable 'y_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "XchlAZPwHpMk"
   },
   "outputs": [],
   "source": [
    "X_data = X.T\n",
    "y_data = y.reshape(1,len(y))\n",
    "\n",
    "assert X_data.shape == (10, 10000)\n",
    "assert y_data.shape == (1, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the network dimension to have **10** input features, **two** **hidden layers** with **9** nodes each, one output node at final layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "t_dEEaJykLSa"
   },
   "outputs": [],
   "source": [
    "layer_dims = [10,9,9,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            #import tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function named placeholders to return two placeholders one for input data as A_0 and one for output data as Y.\n",
    "- Set the datatype of placeholders as float64\n",
    "- parameters - num_features\n",
    "- Returns - A_0 with shape (num_feature, None) and Y with shape(1,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "4kp4_9kS58CT"
   },
   "outputs": [],
   "source": [
    "def placeholders(num_features):\n",
    "    A_0 = tf.placeholder(dtype = tf.float64, shape = ([num_features,None]))\n",
    "    Y = tf.placeholder(dtype = tf.float64, shape = ([1,None]))\n",
    "    return A_0,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define function named initialize_parameters_deep() to initialize weights and bias for each layer\n",
    "- Use tf.get_variable to initialise weights and bias, set datatype as float64\n",
    "- Make sure you are using xavier initialization for weigths and initialize bias to zeros\n",
    "- Parameters - layer_dims\n",
    "- Returns - dictionary of weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "W3l_vyXVkrlw"
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    tf.set_random_seed(1)\n",
    "    L = len(layer_dims)\n",
    "    parameters = {}\n",
    "    for l in range(1,L):\n",
    "        ###Start code here\n",
    "        parameters['W' + str(l)] = \n",
    "        parameters['b' + str(l)] = \n",
    "        #End code\n",
    "    return parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functon named linear_forward_prop() to define forward propagation for a given layer.\n",
    "- parameters: A_prev(output from previous layer), W(weigth matrix of current layer), b(bias vector for current layer),activation(type of activation to be used for out of current layer)  \n",
    "- returns: A(output from the current layer)\n",
    "- Use relu activation for hidden layers and for final output layer return the output unactivated i.e if activation is sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "SKbSUt5g4toV"
   },
   "outputs": [],
   "source": [
    "def linear_forward_prop(A_prev,W,b, activation):\n",
    "    Z = tf.add(tf.matmul(W, A_prev), b)\n",
    "    if activation == \"sigmoid\":\n",
    "        A = Z\n",
    "    elif activation == \"relu\":\n",
    "        A = tf.nn.relu(Z)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define forward propagation for entire network as l_layer_forward()\n",
    "- Parameters: A_0(input data), parameters(dictionary of weights and bias)\n",
    "- returns: A(output from final layer)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Cdks1w__k-Ei"
   },
   "outputs": [],
   "source": [
    "def l_layer_forwardProp(A_0, parameters):\n",
    "    A = A_0\n",
    "    L = len(parameters)//2\n",
    "    for l in range(1,L):\n",
    "        A_prev = A\n",
    "    ###Start code\n",
    "        A =                  #call linear forward prop with relu activation\n",
    "    A =                      #call linear forward prop with sigmoid activation\n",
    "    ###End code\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define the cost function\n",
    "- parameters:\n",
    "  - Z_final: output fro final layer\n",
    "  - Y: actual output\n",
    "  - parameters: dictionary of weigths and bias\n",
    "  - regularization : boolean\n",
    "  - lambd: regularization parameter\n",
    "- First define the original cost using tensoflow's sigmoid_cross_entropy function\n",
    "- If **regularization == True** add regularization term to original cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_cost(Z_final, Y , parameters, regularization = False, lambd = 0):\n",
    "    cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=Z_final,labels=Y)\n",
    "    if regularization:\n",
    "        reg_term = 0\n",
    "        L = len(parameters)//2\n",
    "        for l in range(1,L+1):\n",
    "            ###Start code\n",
    "            reg_term +=                #add L2 loss term\n",
    "            ###End code\n",
    "        cost = cost + (lambd/2) * reg_term\n",
    "    return tf.reduce_mean(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the function to generate mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def random_samples_minibatch(X, Y, batch_size, seed = 1):\n",
    "    np.random.seed(seed)\n",
    "    ###Start code\n",
    "    m =                                            #number of samples\n",
    "    num_batches =                                 #number of batches derived from batch_size\n",
    "    ###End code\n",
    "    indices =                                   # generate ramdom indicies\n",
    "    shuffle_X = X[:,indices]\n",
    "    shuffle_Y = Y[:,indices]\n",
    "    mini_batches = []\n",
    "    \n",
    "    #generate minibatch\n",
    "    for i in range(num_batches):\n",
    "        X_batch = \n",
    "        Y_batch = \n",
    "        \n",
    "        assert X_batch.shape == (X.shape[0], batch_size)\n",
    "        assert Y_batch.shape == (Y.shape[0], batch_size)\n",
    "        \n",
    "        mini_batches.append((X_batch, Y_batch))\n",
    "    \n",
    "    #generate batch with remaining number of samples\n",
    "    if m % batch_size != 0:\n",
    "        X_batch = \n",
    "        Y_batch = \n",
    "        mini_batches.append((X_batch, Y_batch))\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "_aBIc5RIRz-q"
   },
   "outputs": [],
   "source": [
    "def model(X_train,Y_train, layer_dims, learning_rate, optimizer ,num_iter, mini_batch_size):\n",
    "    tf.reset_default_graph()\n",
    "    num_features, num_samples = X_train.shape\n",
    "    ###Start code\n",
    "    A_0, Y =                         #call placeholder function to initialize placeholders A_0 and Y\n",
    "    parameters =                    #Initialse Weights and bias using initialize_parameters\n",
    "    Z_final =                       #call the function l_layer_forwardProp() to define the final output\n",
    "    \n",
    "    cost =                          #call the final_cost function with regularization set TRUE\n",
    "    ###End code\n",
    "    \n",
    "    ###Start code\n",
    "    if optimizer == \"momentum\":\n",
    "        train_net =                  #call tensorflow's momentum optimizer with momentum = 0.9\n",
    "    elif optimizer == \"rmsProp\":\n",
    "        train_net =                   #call tensorflow's RMS optimiser with decay = 0.999\n",
    "    elif optimizer == \"adam\":\n",
    "        train_net =                  ##call tensorflow's adam optimizer with beta1 = 0.9, beta2 = 0.999\n",
    "    ###End code\n",
    "    seed = 1\n",
    "    num_minibatches = int(num_samples / mini_batch_size)\n",
    "    init = tf.global_variables_initializer()\n",
    "    costs = []\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(num_iter):\n",
    "            epoch_cost = 0\n",
    "            ###Start code\n",
    "            mini_batches =               #call random_sample_minibatch to return minibatches\n",
    "            ###End code\n",
    "            seed = seed + 1\n",
    "            \n",
    "            #perform gradient descent for each mini-batch\n",
    "            for mini_batch in mini_batches:\n",
    "                ###Start code\n",
    "                X_batch, Y_batch =             #assign minibatch\n",
    "                ###End code\n",
    "                _,mini_batch_cost = sess.run([train_net, cost], feed_dict={A_0: X_batch, Y: Y_batch})\n",
    "                epoch_cost += mini_batch_cost/num_minibatches\n",
    "            \n",
    "            if epoch % 2 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "            if epoch % 10 == 0:\n",
    "                print(epoch_cost)\n",
    "        with open(\"output.txt\", \"w+\") as file:\n",
    "            file.write(\"%f\" % epoch_cost)\n",
    "        plt.ylim(0 ,2, 0.0001)\n",
    "        plt.xlabel(\"epoches per 2\")\n",
    "        plt.ylabel(\"cost\")\n",
    "        plt.plot(costs)\n",
    "        plt.show()\n",
    "        params = sess.run(parameters)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the method model_with_minibatch() with learning rate 0.001, **optimizer = momentum** num_iter = 100 and minibatch 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_momentum = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the method model_with_minibatch() with learning rate 0.001, **optimizer = rmsProp** num_iter = 100 and minibatch 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_rms = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the method model_with_minibatch() with learning rate 0.001, **optimizer = adam** num_iter = 100 and minibatch 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_adam = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "DNN_hands_on_solution.ipynb",
   "provenance": [
    {
     "file_id": "1i6MJxD7kd04W6ojetXVPsNUeA4qGg6Zq",
     "timestamp": 1525158878710
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
